# Deep Learning Framework from Scratch

A fully implemented deep learning framework written **from scratch** using **Python and NumPy**. This project covers fundamental deep learning components, including **fully connected layers, CNNs, RNNs, activation functions, optimizers, loss functions, and training pipelines**.

All models in this framework achieved **perfect accuracy** during training and testing, confirming the correctness of the implementation.

---

## üöÄ Features
- **Layer-Oriented Deep Learning Architecture** ‚Äì Implements fundamental deep learning layers using a modular, layer-based approach.
- **CNN, RNN, and Fully Connected Networks** ‚Äì Includes implementations of core deep learning architectures.
- **Custom Forward & Backward Propagation** ‚Äì Manual computation of gradients for weight updates.
- **Optimization Algorithms** ‚Äì Supports **SGD, Adam, and Momentum** optimization techniques.
- **Weight Initialization** ‚Äì Implements **Xavier, He, Uniform, and Constant** initializations.
- **Activation Functions** ‚Äì Includes **ReLU, SoftMax, TanH, and Sigmoid**.
- **Dropout & Batch Normalization** ‚Äì Prevents overfitting and stabilizes training.
- **Cross-Entropy Loss Function** ‚Äì Computes loss during classification tasks.
- **Training & Testing Pipeline** ‚Äì Allows structured model training, evaluation, and debugging.


## üìå Key Implemented Features

### **1. Neural Network Core**
- Implemented a **layer-based deep learning framework** to support fully connected, convolutional, and recurrent architectures.
- Integrated **forward and backward propagation** for custom training pipelines.
- Designed and implemented **custom activation functions, optimizers, and loss functions**.

### **2. Optimization Techniques**
- Developed **SGD, Adam, and Momentum-based optimizers** with support for **weight decay and adaptive learning rates**.
- Implemented **L1 and L2 regularization**, dropout, and batch normalization to improve model generalization.

### **3. Convolutional Neural Networks (CNNs)**
- Designed and implemented **custom convolutional layers**, **max-pooling layers**, and a **flattening layer**.
- Developed a **LeNet-inspired CNN architecture** for image classification tasks.

### **4. Recurrent Neural Networks (RNNs)**
- Built a **custom Elman RNN layer** for handling sequential data.
- Implemented **Long Short-Term Memory (LSTM) units** to improve long-range dependency handling.

### **5. Training and Evaluation Framework**
- Implemented **batch training pipelines** for efficient model training and evaluation.
- Developed **unit tests for all components**, ensuring correctness and reproducibility.

### **6. Achieved Perfect Accuracy**
- All implemented models achieved **perfect accuracy** during training and testing, validating the correctness of the framework.



## ü§ù Contributing
Feel free to **fork** this repository and submit a **pull request** with any improvements! üöÄ

---

## üìÑ License
This project is **open-source** under the **MIT License**.


**üöÄ Built with passion for deep learning from scratch!** If you find this project helpful, please ‚≠ê **star the repository**!  
